{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 2,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
       "<pyspark.context.SparkContext at 0x7f7b14276dd8>"
      ]
     },
     "execution_count": 2,
=======
       "<pyspark.context.SparkContext at 0x7f4fec6132e8>"
      ]
     },
     "execution_count": 14,
>>>>>>> - adapted to Python 3
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "import random\n",
=======
>>>>>>> - adapted to Python 3
    "sc = pyspark.SparkContext(appName=\"sparknbs\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "# RDD basics"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "#### [Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "This notebook will introduce three basic but essential Spark operations. Two of them are the *transformations* `map` and `filter`. The other is the *action* `collect`. At the same time we will introduce the concept of *persistence* in Spark.    "
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "## Getting the data and creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "As we did in our first notebook, we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file that we downloaded locally."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "Now we can use this file to create our RDD."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 5,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [],
   "source": [
    "data_file = \"../nb1-rdd-creation/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "## The `filter` transformation"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition. More concretely, a function is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return `True`."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "For example, imagine we want to count how many `normal.` interactions we have in our dataset. We can filter our `raw_data` RDD as follows.  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 2,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "Now we can count how many elements we have in the new RDD."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 3,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97278 'normal' interactions\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
      "Count completed in 5.951 seconds\n"
=======
      "Count completed in 1.223 seconds\n"
>>>>>>> - adapted to Python 3
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "print \"There are {} 'normal' interactions\".format(normal_count)\n",
    "print \"Count completed in {} seconds\".format(round(tt,3))"
=======
    "print(\"There are {} 'normal' interactions\".format(normal_count))\n",
    "print(\"Count completed in {} seconds\".format(round(tt,3)))"
>>>>>>> - adapted to Python 3
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "Remember from notebook 1 that we have a total of 494021 in our 10 percent dataset. Here we can see that 97278 contain the `normal.` tag word.  "
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "Notice that we have measured the elapsed time for counting the elements in the RDD. We have done this because we wanted to point out that actual (distributed) computations in Spark take place when we execute *actions* and not *transformations*. In this case `count` is the action we execute on the RDD. We can apply as many transformations as we want on a our RDD and no computation will take place until we call the first action that, in this case takes a few seconds to complete."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "## The `map` transformation"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "By using the `map` transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 4,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
      "Parse completed in 1.715 seconds\n",
      "[u'0',\n",
      " u'tcp',\n",
      " u'http',\n",
      " u'SF',\n",
      " u'181',\n",
      " u'5450',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'1',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'0',\n",
      " u'8',\n",
      " u'8',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'1.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'9',\n",
      " u'9',\n",
      " u'1.00',\n",
      " u'0.00',\n",
      " u'0.11',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'0.00',\n",
      " u'normal.']\n"
=======
      "Parse completed in 0.026 seconds\n",
      "['0',\n",
      " 'tcp',\n",
      " 'http',\n",
      " 'SF',\n",
      " '181',\n",
      " '5450',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '1',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '0',\n",
      " '8',\n",
      " '8',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '9',\n",
      " '9',\n",
      " '1.00',\n",
      " '0.00',\n",
      " '0.11',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " '0.00',\n",
      " 'normal.']\n"
>>>>>>> - adapted to Python 3
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "print \"Parse completed in {} seconds\".format(round(tt,3))\n",
=======
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))\n",
>>>>>>> - adapted to Python 3
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "Again, all action happens once we call the first Spark *action* (i.e. *take* in this case). What if we take a lot of elements instead of just the first few?  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 5,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
      "Parse completed in 8.629 seconds\n"
=======
      "Parse completed in 1.256 seconds\n"
>>>>>>> - adapted to Python 3
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(100000)\n",
    "tt = time() - t0\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "print \"Parse completed in {} seconds\".format(round(tt,3))"
=======
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))"
>>>>>>> - adapted to Python 3
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "We can see that it takes longer. The `map` function is applied now in a  distributed way to a lot of elements on the RDD, hence the longer execution time."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "### Using `map` and predefined functions"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "Of course we can use predefined functions with `map`. Imagine we want to have each element in the RDD as a key-value pair where the key is the tag (e.g. *normal*) and the value is the whole list of elements that represents the row in the CSV formatted file. We could proceed as follows.    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 6,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
      "(u'normal.',\n",
      " [u'0',\n",
      "  u'tcp',\n",
      "  u'http',\n",
      "  u'SF',\n",
      "  u'181',\n",
      "  u'5450',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'1',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'0',\n",
      "  u'8',\n",
      "  u'8',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'1.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'9',\n",
      "  u'9',\n",
      "  u'1.00',\n",
      "  u'0.00',\n",
      "  u'0.11',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'0.00',\n",
      "  u'normal.'])\n"
=======
      "('normal.',\n",
      " ['0',\n",
      "  'tcp',\n",
      "  'http',\n",
      "  'SF',\n",
      "  '181',\n",
      "  '5450',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '1',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '0',\n",
      "  '8',\n",
      "  '8',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '9',\n",
      "  '9',\n",
      "  '1.00',\n",
      "  '0.00',\n",
      "  '0.11',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  '0.00',\n",
      "  'normal.'])\n"
>>>>>>> - adapted to Python 3
     ]
    }
   ],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "That was easy, wasn't it?"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "In our notebook about working with key-value pairs we will use this type of RDDs to do data aggregations (e.g. count by key)."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "## The `collect` action"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "So far we have used the actions `count` and `take`. Another basic action we need to learn is `collect`. Basically it will get all the elements in the RDD into memory for us to work with them. For this reason it has to be used with care, specially when working with large RDDs.  "
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "An example using our raw data.    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 9,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
      "Data collected in 17.927 seconds\n"
=======
      "Data collected in 2.61 seconds\n"
>>>>>>> - adapted to Python 3
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "print \"Data collected in {} seconds\".format(round(tt,3))"
=======
    "print(\"Data collected in {} seconds\".format(round(tt,3)))"
>>>>>>> - adapted to Python 3
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "That took longer as any other action we used before, of course. Every Spark worker node that has a fragment of the RDD has to be coordinated in order to retrieve its part, and then *reduce* everything together.    "
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "As a last example combining all the previous, we want to collect all the `normal` interactions as key-value pairs.   "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 13,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
>>>>>>> - adapted to Python 3
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
      "Data collected in 12.485 seconds\n",
      "There are 97278 normal interactions\n"
=======
      "Data collected in 2.999 seconds\n",
      "There are 97278 'normal' interactions\n"
>>>>>>> - adapted to Python 3
     ]
    }
   ],
   "source": [
    "# get data from file\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
=======
    "data_file = \"../nb1-rdd-creation/kddcup.data_10_percent.gz\"\n",
>>>>>>> - adapted to Python 3
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "# parse into key-value pairs\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "\n",
    "# filter normal key interactions\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# collect all\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(all_normal)\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "print \"Data collected in {} seconds\".format(round(tt,3))\n",
    "print \"There are {} 'normal' interactions\".format(normal_count)"
=======
    "print(\"Data collected in {} seconds\".format(round(tt,3)))\n",
    "print(\"There are {} 'normal' interactions\".format(normal_count))"
>>>>>>> - adapted to Python 3
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "metadata": {},
=======
   "metadata": {
    "deletable": true,
    "editable": true
   },
>>>>>>> - adapted to Python 3
   "source": [
    "This count matches with the previous count for `normal` interactions. The new procedure is more time consuming. This is because we retrieve all the data with `collect` and then use Python's `len` on the resulting list. Before we were just counting the total number of elements in the RDD by using `count`.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "version": "3.5.2"
=======
   "version": "3.5.3"
>>>>>>> - adapted to Python 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
