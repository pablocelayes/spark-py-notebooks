{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 18,
=======
   "execution_count": 10,
>>>>>>> - adapted to Python 3
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
       "<pyspark.context.SparkContext at 0x7f603a260780>"
      ]
     },
     "execution_count": 18,
=======
       "<pyspark.context.SparkContext at 0x7fa1639ba160>"
      ]
     },
     "execution_count": 10,
>>>>>>> - adapted to Python 3
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
    "import random\n",
=======
>>>>>>> - adapted to Python 3
    "sc = pyspark.SparkContext(appName=\"sparknbs\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# RDD creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### [Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook we will introduce two different ways of getting data into the basic Spark data structure, the **Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements. All work in Spark is expressed as either creating new RDDs, transforming existing RDDs, or calling actions on RDDs to compute a result. Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The reference book for these and other Spark related topics is *Learning Spark* by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The KDD Cup 1999 competition dataset is described in detail [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Getting the data files  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this notebook we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a *Gzip* file that we will download locally.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a RDD from a file  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The most common way of creating an RDD is to load it from a file. Notice that Spark's `textFile` can handle compressed files directly.    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 8,
=======
   "execution_count": 2,
>>>>>>> - adapted to Python 3
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have our data file loaded into the `raw_data` RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Without getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD.  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 9,
=======
   "execution_count": 3,
>>>>>>> - adapted to Python 3
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
     "execution_count": 9,
=======
     "execution_count": 3,
>>>>>>> - adapted to Python 3
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also check the first few entries in our data.  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 10,
=======
   "execution_count": 4,
>>>>>>> - adapted to Python 3
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
     "execution_count": 10,
=======
     "execution_count": 4,
>>>>>>> - adapted to Python 3
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the following notebooks, we will use this raw data to learn about the different Spark transformations and actions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating and RDD using `parallelize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Another way of creating an RDD is to parallelize an already existing list.  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 11,
=======
   "execution_count": 5,
>>>>>>> - adapted to Python 3
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = range(100)\n",
    "\n",
    "data = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we did before, we can `count()` the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 12,
=======
   "execution_count": 6,
>>>>>>> - adapted to Python 3
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
     "execution_count": 12,
=======
     "execution_count": 6,
>>>>>>> - adapted to Python 3
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As before, we can access the first few elements on our RDD.  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
   "execution_count": 13,
=======
   "execution_count": 7,
>>>>>>> - adapted to Python 3
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
<<<<<<< 96d08e8d4cf03874956ae1d927d6f734ce2c0c4f
     "execution_count": 13,
=======
     "execution_count": 7,
>>>>>>> - adapted to Python 3
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
